# Energy Load Forecasting with Deep Learning

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.8%2B-orange)
![License](https://img.shields.io/badge/License-MIT-green)

Advanced multi-step time series forecasting for energy load prediction using Sequence-to-Sequence LSTM architecture. This project demonstrates sophisticated deep learning techniques for predicting hourly energy consumption with 80%+ improvement over baseline methods.

## ðŸš€ Key Features

- **Multi-step Forecasting**: Predicts 24 hours of energy load from 72 hours of historical data
- **Synthetic Data Generation**: Realistic energy consumption patterns with trend and seasonality
- **Advanced Architecture**: Seq2Seq LSTM with encoder-decoder structure
- **Production Ready**: Comprehensive preprocessing, training, and evaluation pipeline
- **Performance**: 80.3% improvement in RMSE over naive baseline

## ðŸ“Š Results

| Model | RMSE | MAPE | Improvement |
|-------|------|------|-------------|
| **LSTM (Ours)** | **5.50** | **3.66%** | **-**
| Naive Baseline | 27.93 | 19.32% | 80.3% better |

## ðŸ—ï¸ Architecture
Input (72 hours) â†’ LSTM Encoder â†’ RepeatVector â†’ LSTM Decoder â†’ Output (24 hours)


- **Input Sequence**: 72 hours of historical energy load data
- **Output Sequence**: 24 hours of future predictions
- **Model**: Encoder-Decoder LSTM with 64 units each
- **Parameters**: 33,857 trainable parameters

## ðŸ“ Project Structure
energy-load-forecasting/
 
â””â”€â”€ energy_forecasting.ipynb   # main file


## ðŸ› ï¸ Installation & Usage

### Prerequisites
- Python 3.8+
- Jupyter Notebook

### Quick Start

1. **Clone the repository**
git clone https://github.com/yourusername/energy-load-forecasting.git
cd energy-load-forecasting

2. **Install dependencies**
pip install -r requirements.txt

3.**Run the notebook**
jupyter notebook energy_forecasting.ipynb

## Dependencies
tensorflow>=2.8.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
jupyter>=1.0.0

## ðŸ“ˆ How It Works
### 1. Data Generation
Synthetic energy load data with:

- Base Load: 100 units constant
- Trend: Gradual increase over time

### Seasonality:

- Daily patterns (morning/evening peaks)
- Weekly patterns (weekend reduction)
- Yearly patterns (seasonal variation)

Noise: Random variations simulating real-world conditions

### 2. Preprocessing Pipeline
- Temporal train/validation/test split (70/15/15)
- MinMax scaling to [0, 1] range
- Sequence creation for Seq2Seq training

Input: 72-hour sequences â†’ Output: 24-hour sequences

### 3. Model Training
Architecture: Encoder-Decoder LSTM

Optimizer: Adam with learning rate 0.001

Callbacks: Early stopping and learning rate reduction

Training: 50 epochs with batch size 32

### 4. Evaluation
Metrics: RMSE and MAPE

Baseline Comparison: Naive forecast (repeat last value)

Visualization: Training history, prediction examples, error analysis

## ðŸŽ¯ Key Implementation Details
### Model Architecture
model = Sequential([
    LSTM(64, return_sequences=False, input_shape=(72, 1)),
    RepeatVector(24),
    LSTM(64, return_sequences=True),
    TimeDistributed(Dense(1))
])
## Training Strategy
- Teacher forcing during training
- Early stopping to prevent overfitting
- Learning rate scheduling for convergence
- Temporal validation to respect time series order

## ðŸ“Š Results Analysis
### Performance Highlights
RMSE: 5.50 (80.3% better than baseline)

MAPE: 3.66% (81.1% better than baseline)

Training Time: ~2-5 minutes on CPU

Inference Speed: Real-time predictions

### Visual Results
- The model successfully captures:
- Daily consumption patterns (peak hours)
- Weekly trends (weekend vs weekday)
- Long-term seasonal variations

## ðŸš€ Future Enhancements
- Add real energy consumption data
- Incorporate external features (temperature, holidays)
- Implement attention mechanism
- Deploy as web service
- Add hyperparameter tuning with Optuna
- Implement Transformer architecture
